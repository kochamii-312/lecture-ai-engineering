# visualize.py
import os
import requests
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from data import train_df
from preprocess import split_into_sentences, merge_comment_columns
from sentiment_classify import SentimentClassifier
from category_classify import CategoryClassifier
from clustering import CommentEmbedder
from dotenv import load_dotenv
from comment_lists import positive_comment_list, negative_comment_list, neutral_comment_list, lecture_content_comment_list, lecture_materials_comment_list, operation_comment_list, others_comment_list

load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

headers = {"Authorization": f"Bearer {HF_TOKEN}"}

# ãƒ¡ãƒ¢ï¼šè¬›ç¾©å›žã”ã¨ã«åˆ†ã‘ã‚‹
# ãƒ¡ãƒ¢ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«ã§å‡ºåŠ›ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
def show_all_visualizations():
    st.header("åˆ†æžçµæžœ")
    st.subheader("æ„Ÿæƒ…åˆ†æž")
    show_setiment_visulization()
    st.divider()
    st.subheader("ã‚«ãƒ†ã‚´ãƒªåˆ¥")
    show_category_visualization()
    st.divider()
    show_dangerous_comment_visualization()

def show_setiment_visulization():
    """
    ã€Œç‰¹ã«è‰¯ã‹ã£ãŸéƒ¨åˆ†ã€ã®æ¬„ã®ã‚³ãƒ¡ãƒ³ãƒˆã¨ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡žå™¨ã®çµæžœã‚’positive_comment_listã«æ ¼ç´ã€
    ã€Œåˆ†ã‹ã‚Šã«ãã‹ã£ãŸç‚¹ã‚„ä¸æº€ãŒã‚ã£ãŸç‚¹ã€ã®æ¬„ã®ã‚³ãƒ¡ãƒ³ãƒˆã¨ã€ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡žå™¨ã®çµæžœã‚’negative_comment_listã«æ ¼ç´ã™ã‚‹
    ãã‚Œãžã‚Œã®listã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã¦è¦ç´„
    """
    from main import df
    positive_comment_list.append(split_into_sentences(df[-5]))
    negative_comment_list.append(split_into_sentences(df[-4]))

    columns = ['comment3_about_teacher', 'comment4_future_suggestions', 'comment5_free']
    train_comment_list = split_into_sentences(merge_comment_columns(train_df, columns))
    comment_list = split_into_sentences(merge_comment_columns(df, columns))
    
    sc = SentimentClassifier()
    sc.train_on(train_comment_list)
    sc.predict_on(comment_list)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    embedder = CommentEmbedder(hf_token=HF_TOKEN)

    # ã‚¯ãƒ©ã‚¹ã‚¿ã‚’çµåˆã—ã¦è¦ç´„
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚³ãƒ¡ãƒ³ãƒˆã®è¦ç´„")
        clusters = embedder.cluster_comments(positive_comment_list, n_clusters=5)
        # ãƒ¡ãƒ¢: LLMã‚’ä½¿ã‚ãšä»£è¡¨ã‚³ãƒ¡ãƒ³ãƒˆã§ã‚‚
        for cluster_id, items in clusters.items():
            summary = embedder.summarize_cluster(items)
            st.text(f"- ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}. {summary}")
    with col2:
        st.subheader("ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚³ãƒ¡ãƒ³ãƒˆã®è¦ç´„")
        clusters = embedder.cluster_comments(negative_comment_list, n_clusters=5)
        # ãƒ¡ãƒ¢: LLMã‚’ä½¿ã‚ãšä»£è¡¨ã‚³ãƒ¡ãƒ³ãƒˆã§ã‚‚
        for cluster_id, items in clusters.items():
            summary = embedder.summarize_cluster(items)
            st.text(f"- ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}. {summary}")
    st.divider()

def show_category_visualization():
    columns = ['comment1_positive, comment2_negative, commet3_about_teacher, comment4_future_suggestions, comment5_free']
    train_comment_list = split_into_sentences(merge_comment_columns(train_df, columns))
    cc = CategoryClassifier()
    cc.train_on(train_comment_list)
    cc.predict_on(positive_comment_list)
    cc.predict_on(negative_comment_list)

    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å‰²åˆ
    comment_lists = [
        lecture_content_comment_list,
        lecture_materials_comment_list,
        operation_comment_list,
        others_comment_list
    ]
    category_names = ["è¬›ç¾©å†…å®¹", "è¬›ç¾©è³‡æ–™", "é‹å–¶", "ãã®ä»–"]
    counts = [len(lst) for lst in comment_lists]

    # Streamlit è¡¨ç¤º
    st.subheader("ðŸ“Šã‚³ãƒ¡ãƒ³ãƒˆã‚«ãƒ†ã‚´ãƒªã®å††ã‚°ãƒ©ãƒ•å¯è¦–åŒ–")

    # å††ã‚°ãƒ©ãƒ•ä½œæˆ
    fig, ax = plt.subplots()
    ax.pie(counts, labels=category_names, autopct='%1.1f%%', startangle=90, counterclock=False)
    ax.axis('equal')  # å††ã‚’æ­£å††ã«

    st.pyplot(fig)

    # ä»¶æ•°ã®ãƒ†ãƒ¼ãƒ–ãƒ«
    st.markdown("### ðŸ“‹ä»¶æ•°ãƒ‡ãƒ¼ã‚¿")
    for name, count in zip(category_names, counts):
        st.write(f"- {name}: {count}ä»¶")

    # é‡è¦åº¦ã‚¹ã‚³ã‚¢ãŒé«˜ã„ä¸Šä½10ä»¶
    st.subheader("ðŸ”ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦ã®é«˜ã„ã‚³ãƒ¡ãƒ³ãƒˆ")
    st.text("ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é‡è¦åº¦ãŒé«˜ã„ã‚³ãƒ¡ãƒ³ãƒˆã‚’10ä»¶ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
    embedder = CommentEmbedder(hf_token=HF_TOKEN)
    tab1, tab2, tab3, tab4 = st.tabs("è¬›ç¾©å†…å®¹", "è¬›ç¾©è³‡æ–™", "é‹å–¶", "ãã®ä»–")
    with tab1:
        st.markdown("### ðŸ“Šã‚«ãƒ†ã‚´ãƒªã€Œè¬›ç¾©å†…å®¹ã€é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        for i, (count, representative) in enumerate(embedder.cluster_and_rank(lecture_content_comment_list), 1):
            st.markdown(f"**{i}. ä»¶æ•°: {count}ä»¶**")
            st.write(f"ä»£è¡¨ã‚³ãƒ¡ãƒ³ãƒˆ: {representative}")
    with tab2:
        st.markdown("### ðŸ“Šã‚«ãƒ†ã‚´ãƒªã€Œè¬›ç¾©è³‡æ–™ã€é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        for i, (count, representative) in enumerate(embedder.cluster_and_rank(lecture_materials_comment_list), 1):
            st.markdown(f"**{i}. ä»¶æ•°: {count}ä»¶**")
            st.write(f"ä»£è¡¨ã‚³ãƒ¡ãƒ³ãƒˆ: {representative}")
    with tab3:
        st.markdown("### ðŸ“Šã‚«ãƒ†ã‚´ãƒªã€Œé‹å–¶ã€é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        for i, (count, representative) in enumerate(embedder.cluster_and_rank(operation_comment_list), 1):
            st.markdown(f"**{i}. ä»¶æ•°: {count}ä»¶**")
            st.write(f"ä»£è¡¨ã‚³ãƒ¡ãƒ³ãƒˆ: {representative}")
    with tab4:
        st.markdown("### ðŸ“Šã‚«ãƒ†ã‚´ãƒªã€Œãã®ä»–ã€é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        for i, (count, representative) in enumerate(embedder.cluster_and_rank(others_comment_list), 1):
            st.markdown(f"**{i}. ä»¶æ•°: {count}ä»¶**")
            st.write(f"ä»£è¡¨ã‚³ãƒ¡ãƒ³ãƒˆ: {representative}")

def show_dangerous_comment_visualization():
    from main import df
    columns = ['comment1_positive, comment2_negative, commet3_about_teacher, comment4_future_suggestions, comment5_free']
    flagged_comments = detect_dangerous_comments(split_into_sentences(merge_comment_columns(df, columns)))
    if flagged_comments != 0:
        st.text(f"âš ï¸ å±é™ºã‚³ãƒ¡ãƒ³ãƒˆ {len(flagged_comments)} ä»¶æ¤œå‡º:")
        for comment, score in flagged_comments:
            st.text(f"- {comment} (score:{score:.2f})")
    else:
        st.text("âœ… å±é™ºã‚³ãƒ¡ãƒ³ãƒˆã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

def detect_dangerous_comments(texts, threshold=0.8):
    api_url = "https://api-inference.huggingface.co/models/joeddav/xlm-roberta-large-xnli"
    label_danger = "å±é™ºã¾ãŸã¯ä¸é©åˆ‡"

    flagged = []
    for text in texts:
        payload = {
            "inputs": text,
            "parameters": {"candidate_labels": [label_danger, "å®‰å…¨"], "multi_label": False}
        }
        response = requests.post(api_url, headers=headers, json=payload)
        result = response.json()
        if result["labels"][0] == label_danger and result["scores"][0] > threshold:
            flagged.append((text, result["scores"][0]))
    if not flagged:
        return 0
    return flagged
